{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, I am Alok Sinha. I am doing NLP Digital Assignment\n"
     ]
    }
   ],
   "source": [
    "document = nlp(\"Hey, I am Alok Sinha. I am doing NLP Digital Assignment\")\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x1a9879e4ef0>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x1a987a24708>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x1a987a24768>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Object :  Hey, I am Alok Sinha. I am doing NLP Digital Assignment\n",
      "Token Object :  ,\n",
      "Token Object :  Assignment\n",
      "Span Object :  , I\n"
     ]
    }
   ],
   "source": [
    "document = nlp(\"Hey, I am Alok Sinha. I am doing NLP Digital Assignment\")\n",
    "print(\"Document Object : \",document.text)\n",
    "print(\"Token Object : \", document[1].text)\n",
    "print(\"Token Object : \", document[-1].text)\n",
    "span = document[1:3]\n",
    "print(\"Span Object : \",span.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-80a3a5019584>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-80a3a5019584>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    w.text for w in document.sents\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "w.text for w in document.sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey, I am Alok Sinha.', 'I am doing NLPP Digital Assignment']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document=nlp(\"Hey, I am Alok Sinha. I am doing NLPP Digital Assignment\")\n",
    "[w.text for w in document.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['while', 'move', 'ourselves', 'themselves', 'using', 'your', 'myself', 'an', 'none', 'between'] \n",
      "\n",
      "\n",
      "['Hi', 'Bye', 'while', 'move', 'ourselves', 'themselves', 'using', 'your', 'myself', 'an'] \n",
      "\n",
      "\n",
      "['while', 'move', 'ourselves', 'themselves', 'using', 'your', 'rather', 'myself', 'â€˜d', 'after'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(list(STOP_WORDS)[:10],\"\\n\\n\")\n",
    "stopwords = ['Hi', 'Bye'] + list(STOP_WORDS)\n",
    "print(stopwords[:10],\"\\n\\n\")\n",
    "stopwords = set(STOP_WORDS) - {'done', 'whether'}\n",
    "print(list(stopwords)[:10],\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hey', 'hey'), (',', ','), ('I', '-PRON-'), ('am', 'be'), ('Alok', 'Alok'), ('Sinha', 'Sinha'), ('.', '.'), ('I', '-PRON-'), ('am', 'be'), ('doing', 'do'), ('NLP', 'NLP'), ('Digital', 'Digital'), ('Assignment', 'Assignment')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Hey, I am Alok Sinha. I am doing NLP Digital Assignment\")\n",
    "print([(w.text, w.lemma_) for w in doc]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hey', 'INTJ'), (',', 'PUNCT'), ('I', 'PRON'), ('am', 'AUX'), ('Alok', 'PROPN'), ('Sinha', 'PROPN'), ('.', 'PUNCT'), ('I', 'PRON'), ('am', 'AUX'), ('doing', 'VERB'), ('NLP', 'PROPN'), ('Digital', 'PROPN'), ('Assignment', 'PROPN')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Hey, I am Alok Sinha. I am doing NLP Digital Assignment\")\n",
    "print([(w.text, w.pos_) for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47707877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aloks\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"We are learning SpaCy\")\n",
    "doc2 = nlp(\"SpaCy is interesting\")\n",
    "\n",
    "span1 = doc1[2:]\n",
    "span2 = doc2[:]\n",
    "\n",
    "print(span1.similarity(span2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44355288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aloks\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"We are learning SpaCy\")\n",
    "doc2 = nlp(\"SpaCy is interesting\")\n",
    "\n",
    "token1 = doc1[2]\n",
    "token2 = doc2[2]\n",
    "\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.0719783   1.6339191  -0.78885114  0.00953311  1.5233612   2.169728\n",
      "  3.1865263   6.54158     4.083664    2.6522727   0.8947581   0.24499565\n",
      " -0.16254652 -0.26027864  2.962055    3.3617415   1.4682095   0.63954735\n",
      " -2.4154153  -0.3853802   0.32902443  0.6798798  -0.81846666 -1.2062532\n",
      " -1.3923755   0.9811786  -1.8924632   1.2768791  -2.3056087   0.5704824\n",
      " -0.68852174  2.0756938  -2.529599    0.2517932   1.7864335   2.8959148\n",
      "  1.0231869   0.13339692 -1.0133132  -0.9631963   2.6283903   0.57186216\n",
      " -3.4949274   2.0814188   1.1319573   2.6330073  -0.24446505  2.8544364\n",
      " -1.2309194  -0.8273595  -3.259078    0.758295   -0.90195197  0.16871995\n",
      " -1.2733622  -1.545179    0.11051518 -0.8739389  -4.1864467   2.2086382\n",
      " -1.0018792   1.6633899   0.49873182  1.7320386  -0.7877495   2.6620927\n",
      "  2.2458131   0.6971954   0.06427774  0.859339   -3.563279    1.484957\n",
      "  0.7712428  -2.3174267   4.4157305   3.0607214  -2.9673805  -1.6662536\n",
      "  1.6584021  -1.1009421  -1.8275089  -0.8837117  -1.2919536   0.38874918\n",
      " -3.6199045  -2.9276202  -0.2319617   0.47766826 -1.3976833  -1.8619741\n",
      "  0.22825938  1.5660205  -1.9018482  -4.995224   -3.6123958  -1.1888475 ]\n"
     ]
    }
   ],
   "source": [
    "# Process some text\n",
    "doc = nlp(\"Hey I am Alok.\")\n",
    "\n",
    "# Access the vector via the token.vector attribute\n",
    "print(doc[0].vector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4339234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aloks\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I am learning Spacy\")\n",
    "doc2 = nlp(\"SpaCy is interesting\")\n",
    "\n",
    "span1 = doc1[2:]\n",
    "span2 = doc2[:]\n",
    "\n",
    "print(span1.similarity(span2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alok Sinha PERSON 11 21\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"My name is Alok Sinha.\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_, ent.start_char, ent.end_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
